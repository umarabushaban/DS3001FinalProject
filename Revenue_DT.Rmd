---
title: "revenue_model"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
#Load libraries
library(tidyverse)
library(rio)
library(plyr)
library(rpart)
library(psych)
library(pROC)
#install.packages("rpart.plot")
library(rpart.plot)
#install.packages("rattle")
library(rattle)
library(caret)
library(C50) #Need this to pass into caret 
library(mlbench)
```

The goal here is really to be able to create a model that can predict if a movie is going to generate at least $5 million in revenue. To do this, we're going to use C5.0, which will construct decision trees in two phases: First, it will generate a larger tree to loosely fit the data, and then it will be 'pruned' down to something more tightly fitting.

Pre-processing:
```{r}
movies = read.csv('./Movies.csv')
movies = movies[1:1000,]
#View(movies)

#Lets add a new column to the dataset that turns movies that made less than $5,000,000 revenue into 0s, and 
#movies that made more than $5,000,000 into 1s. This is what we want our model to be able to classify.

movies = movies %>%
  mutate(new_rev = case_when(revenue > 5000000 ~ "moreThan5Mil",
                             revenue < 5000000 ~ "LessThan5Mil"
                             #TRUE ~ 0 #check this
                             ))

```

```{r}
#Change to factors
movies[2] = as.factor(movies$original_language)
movies[25] = as.factor(movies$new_rev)

movies$original_language <- fct_collapse(movies$original_language,
                        English = "English",
                        Other = c("Italian","French","German","Japanese")
                        )
```

Splitting the Data
```{r}
#There is not a easy way to create 3 partitions using the createDataPartitions
#so we are going to use it twice. Mostly because we want to stratify on the variable we are working to predict. 
part_index_1 <- caret::createDataPartition(movies$new_rev,
                                           times=1,
                                           p = 0.70,
                                           groups=1,
                                           list=FALSE)

train <- movies[part_index_1, ]
tune_and_test <- movies[-part_index_1, ]

#The we need to use the function again to create the tuning set 

tune_and_test_index <- createDataPartition(tune_and_test$new_rev,
                                           p = .5,
                                           list = FALSE,
                                           times = 1)

tune <- tune_and_test[tune_and_test_index, ]
test <- tune_and_test[-tune_and_test_index, ]


dim(train)
dim(test)# these will be slightly off because the data set isn't perfectly even
#buts its not a issue. 
dim(tune)

```

```{r}
# Choose the features and classes

features <- train[,c(-4,-(8:24),-25)] #dropping 4 and 25. 4 is the original revenue count, which could be used to perfectly predict the 0 or 1 value, which is column 25
```

```{r}
target <- train$new_rev

str(features)
str(target)

#Cross validation process 

fitControl <- trainControl(method = "repeatedcv",
                          number = 10,
                          repeats = 5, 
                          returnResamp="all",
                          classProbs = TRUE,
                          allowParallel = TRUE) 

# number - number of folds
# repeats - number of times the CV is repeated, here it's 5 take the average of
# those 5 repeats

# Grid search options for each of the models available in CARET
# http://topepo.github.io/caret/train-models-by-tag.html#tree-based-model

grid <- expand.grid(.winnow = c(TRUE,FALSE), 
                    .trials=c(1,5,10,15,20), 
                    .model="tree")

#expand.grid - series of options that are available for model training

#winnow - whether to reduce the feature space -  Works to remove unimportant 
#features but it doesn't always work, in the above we are winnowing.  

#trails - number of boosting iterations to try, 1 indicates a single model 
#model - type of ml model

set.seed(1984)
movies_mdl <- train(x=features,
                y=target,
                method="C5.0",
                tuneGrid=grid,
                trControl=fitControl,
                verbose=TRUE)

movies_mdl #provides us the hyper-parameters that were selected through the grid
# search process. 

View(movies_mdl$pred)

# visualize the re-sample distributions
xyplot(movies_mdl,type = c("g", "p", "smooth"))

varImp(movies_mdl)

```
Let's use the model to predict and the evaluate the performance
```{r}

movies_pred_tune = predict(movies_mdl,tune, type= "raw")

#View(as_tibble(movies_pred_tune))


#Lets use the confusion matrix

(movies_eval <- confusionMatrix(as.factor(movies_pred_tune), 
                as.factor(tune$new_rev), 
                dnn=c("Prediction", "Actual"), 
                mode = "sens_spec"))

table(tune$new_rev)

(movies_pred_tune_p = predict(movies_mdl,tune,type= "prob"))

```

Now we can optimize the model using the tune dataset. We can create a few models and see which ones are the most optimal in terms of their metrics, such as accuracy and sensitivity.

# Let's make some changes and see if we can improve

```{r}
#Cross Validation Process, changing method for CV and adding a different metric for optimization 

library(MLmetrics)
f1 <- function(data, lev = NULL, model = NULL) {
  f1_val <- F1_Score(y_pred = data$pred, y_true = data$obs, positive = lev[1])
  c(F1 = f1_val)
}
#source: https://stackoverflow.com/questions/37666516/caret-package-custom-metric

fitControl_2 <- trainControl(method = "LGOCV",
                          number = 10, 
                          returnResamp="all",
                          classProbs = TRUE,
                          allowParallel = TRUE,
                          summaryFunction = f1) 

# grid search, increasing the boosting rounds 
grid_2 <- expand.grid(.winnow = c(TRUE,FALSE), 
                    .trials=c(10,15,20,25,30), 
                    .model="tree")

# training model
set.seed(1984)
movies_mdl_2 <- train(x=features,
                y=target,
                method="C5.0",
                tuneGrid=grid_2,
                metric="F1",
                trControl=fitControl_2)


movies_mdl
movies_mdl_2

```

# Evaluation of model 2

```{r}
movies_pred_tune_2 = predict(movies_mdl_2,tune, type= "raw")


#Lets use the confusion matrix

(model_eval_2 <- confusionMatrix(as.factor(movies_pred_tune_2), 
                as.factor(tune$new_rev), 
                dnn=c("Prediction", "Actual"), 
                mode = "sens_spec"))
model_eval_2
movies_eval

```

# Now give the changes we made above let's final the model and check the metrics
# output on the test file. 

# Final Evaluation 
```{r}
#This is actually pretty good. Increased the sensitivity by more than 10% without losing much accuracy

movies_pred_test = predict(movies_mdl_2,test, type= "raw")

View(as_tibble(movies_pred_test))


#Using the confusion matrix:

confusionMatrix(as.factor(movies_pred_test), 
                as.factor(test$new_rev), 
                dnn=c("Prediction", "Actual"), 
                mode = "sens_spec")


(movies_pred_tune_p = predict(movies_mdl,test,type= "prob"))
```
It seems that the final model is decent. The accuracy stayed relatively similar, but overall as we progressed through these models, the sentivitiy got a lot higher. 94% sensitivity.



